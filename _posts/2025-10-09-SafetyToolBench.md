---
title: "From Forensics to Pre-Flight Checks"
categories: AgenticAI, Safety, GenerativeAI
author: "Vidya Bhandary"
meta: "#AgenticAI #Safety #GenAI"
date: 2025-10-09
---

Making Tool-Using AI Agents Saferâ€”Before They Act

ğŸ¤– Agents no longer just talkâ€”they do:

 - ğŸ’¸ move money,
 - ğŸ“… schedule,
 -  ğŸ”Œ control devices,
 -  âœ‰ï¸ send messages.

âš ï¸ That power raises risksâ€”

 -  ğŸ”’ privacy leaks,
 -  ğŸ’° losses,
 -  ğŸ©¹ harm,
 -  âš–ï¸ bias.

Most tests flag issues after execution. We need prospective safety: checks before tools run.

Enter:

- 1ï¸âƒ£ SafeToolBench: 1,200 adversarial instructions across 16 domains; 4 risk types (Privacy, Property, Physical, Bias). Realistic, parameterized plans (recipient, amount, temperature).
- 2ï¸âƒ£ SafeInstructTool: a 3-perspective, 9-dimension pre-execution score.
- ğŸ‘¤ User Instruction â†’ data sensitivity, harmfulness, urgency, frequency.
- ğŸ› ï¸ Tool Itself â†’ key sensitivity, operation type (read/modify/irreversible), impact scope (userâ†’system-wide).
- ğŸ”— Joint Instructionâ€“Tool â†’ alignment (is the call faithful to the intent?) and value sensitivity (are parameter values safe for this context?).

ğŸ” Why the joint view matters

 -  â€œSend medical report to the groupâ€ â†’ ğŸš« flags before PHI leaks.
 -  â€œSet AC to 0 Â°C overnightâ€ â†’ â„ï¸ blocks unsafe values.
 -  â€œPay John Michaelâ€ vs â€œJohn Davidâ€ â†’ ğŸ”„ forces disambiguation.


ğŸ“Š Results

SafeInstructTool beats prompt-only baselines (CoT, self-consistency), with biggest gains for open-source models and multi-app plans.

- ğŸ”’ Hardest risks: Privacy + Physical (subtle, context-heavy).
- ğŸ§© Largest error source (~47%): the Joint view.

âš™ï¸ How it works

 -  ğŸ—‚ï¸ Build an API Safety Registry (pre-score tools for key sensitivity, reversibility, blast radius).
 -  ğŸ§® Score each requestâ€™s instruction (sensitivity/harm/urgency/frequency) and each tool call (alignment/value sensitivity).
 -  â• Aggregate: S = Instruction + max(Tool + Joint).
 -  ğŸš§ If S â‰¥ 10, block or escalate (2-person approval, explicit confirm, or safer plan).

ğŸ“˜ Playbook

 -  ğŸ—‚ï¸ Stand up the Registry; refresh via policy reviews & postmortems.
 -  ğŸš¦ Gate on S â‰¥ 10; log scores for audits.
 -  ğŸ”— Treat joint risks as first-class: verify recipients, bound parameters, check context.
 -  ğŸ”„ Prefer reversible actions: draft/hold/soft-delete; confirm irreversible ops.
 -  ğŸ  Codify commonsense for Home/IoT: temp, time-of-day, occupancy, power limits.

ğŸŒ Why it matters

- âœ… Safety moves upstreamâ€”pre-flight checks âœˆï¸, not post-hoc forensics.
- âœ… Quantified scores enable approvals, reduce â€œoops-sent/charged/deleted,â€ and shrink blast radius.
- âœ… Separates capability (tools) from governance (risk scoring + gates).


ğŸ¯ Bottom line

Tool-using copilots are powerful. With prospective safety, you stop bad actions before they happenâ€”and scale trust with capability.

#AIResearch #MachineLearning #AIAlignment #TechStrategy #ArtificialIntelligence #ProductDevelopment #AIGovernance #TechLeadership #Innovation

Reference:

[SafetyToolBench](https://arxiv.org/abs/2509.07315)

