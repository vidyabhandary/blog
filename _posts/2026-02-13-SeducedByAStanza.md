---
title: "Seduced by a Stanza"
categories: LLMs, AI, AISecurity, Safety
author: "Vidya Bhandary"
meta: "#AI #LLM #AgenticAI"
date: 2026-02-13
---

# ğŸ§  Seduced by a Stanza

![](https://raw.githubusercontent.com/vidyabhandary/blog/refs/heads/master/images/Adversarial%20Poetry.png)

Looks like AI is not immune to poetry !

A recent paper, identifies a surprisingly strong and systematic weakness in how modern LLMs enforce safety.

The core finding is simple:

- âš ï¸ Harmful requests rewritten as poetry are likely to bypass safety mechanisms.
- âš ï¸ A Single-Turn Attack, Not Prompt Engineering !

Attack details:

- ğŸ”¸ Single-turn interaction only
- ğŸ”¸ No role-play, personas, or fictional framing
- ğŸ”¸ No system prompt access
- ğŸ”¸ Provider-default configurations throughout

Despite these limits, poetic prompts consistently return unsafe outputs. 

ğŸ§© Style, Not Content is the key and the effect is large and reproducible.

Across 25 frontier models from nine providers:

- ğŸ”¸ Prose baseline attack success rate: ~8%
- ğŸ”¸ Poetry attack success rate: ~43%

For a curated set of 20 adversarial poems:

- ğŸ”¸ Average success rate: 62%
- ğŸ”¸ Several models exceed 90%
- ğŸ”¸ One flagship model refuses none of the poetic prompts

These results rival, and in some cases exceed, state-of-the-art jailbreak techniques.


The vulnerability is not tied to a specific model family or alignment method. It appears across:

- ğŸ”¸ Proprietary and open-weight models
- ğŸ”¸ Multiple architectures 

It also spans safety domains, including:

- ğŸ”¸ Cyber offense and malware-related tasks
- ğŸ”¸ Harmful manipulation and fraud
- ğŸ”¸ Privacy violations
- ğŸ”¸ Loss-of-control scenarios

This pattern points to a general alignment failure, not a domain-specific loophole.

ğŸ§© This attack is also not dependent on human creativity. The authors converted 1,200 ML Commons benchmark prompts into poetry using a single standardized meta-prompt. No handcrafted poems required.

âš¡ Key takeaway:

Poetry is not an edge case. It is a normal, benign, and widely used mode of language. Safety mechanisms in AI need to become invariant to style, and narrative.

Technical insights from Lean2Lead, Lean2Lead Pune, Sonu Batra

Reference: 

[Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304v2)


#LLMs #Safety #WomenInTech #Governance #AI 
